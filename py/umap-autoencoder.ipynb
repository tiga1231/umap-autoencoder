{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn\n",
    "# !pip install tqdm\n",
    "# !pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython import display\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import re\n",
    "import math\n",
    "from time import time\n",
    "import os, shutil\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.FashionMNIST('./dataset/', train=True, download=True)\n",
    "dataset_test = datasets.FashionMNIST('./dataset/', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 10000\n",
    "imgs = dataset_train.data[:n].float().numpy()\n",
    "imgs /= 255.0\n",
    "labels = dataset_train.targets[:n].numpy().astype(dtype=np.uint8)\n",
    "\n",
    "umap = UMAP(metric='l1', min_dist=0.4)\n",
    "embeddings = umap.fit_transform(imgs.reshape([-1,28*28]))\n",
    "\n",
    "print(\n",
    "    imgs.shape, embeddings.shape, labels.shape, imgs.dtype, embeddings.dtype, labels.dtype\n",
    ")\n",
    "\n",
    "## save computed embeddings and dataset\n",
    "!mkdir ./tmp\n",
    "embeddings.tofile('tmp/embeddings.bin')\n",
    "labels.tofile('tmp/labels.bin')\n",
    "imgs.tofile('tmp/imgs.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load computed embeddings and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = np.fromfile('tmp/embeddings.bin', dtype=np.float32).reshape([-1,2])\n",
    "# imgs = np.fromfile('tmp/imgs.bin', dtype=np.float32).reshape([-1,28,28])\n",
    "# labels = np.fromfile('tmp/labels.bin', dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7,5])\n",
    "plt.scatter(embeddings[:,0], embeddings[:,1], s=2, c=labels, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.axis('square')\n",
    "plt.title('embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, imgs, embeddings, labels):\n",
    "        self.imgs = imgs\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.imgs[i], self.embeddings[i], self.labels[i]\n",
    "    \n",
    "nTrain, nTest = 9000,1000\n",
    "train_dataset = EmbeddingDataset(imgs[:nTrain], embeddings[:nTrain], labels[:nTrain])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = EmbeddingDataset(imgs[-nTest:], embeddings[-nTest:], labels[-nTest:])\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def flatten(ndim):\n",
    "    def f(x):\n",
    "        return x.view(-1, ndim)\n",
    "    return f\n",
    "\n",
    "def postprocess(x):\n",
    "    return x.view(-1, 28, 28)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            Lambda(flatten(28*28)),\n",
    "#             Lambda(preprocess),\n",
    "#             nn.Conv2d(1,20,3),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Linear(784,250),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(250,100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(100,10),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(10,2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,784),\n",
    "            nn.Sigmoid(),\n",
    "            Lambda(postprocess),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions for vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowGrid(imgs):\n",
    "    im = make_grid(imgs.cpu().detach().unsqueeze(1), padding=2, pad_value=1).permute(1,2,0)\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "model = Autoencoder()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "kl = nn.KLDivLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "## show progress bar here\n",
    "nepoch = 30\n",
    "epochBar = tqdm(range(nepoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gs = GridSpec(2,3,width_ratios=[1.5,1.5,1])\n",
    "lossHistory = []\n",
    "for epoch in epochBar:\n",
    "    for imgs, embeddings, labels in train_loader:\n",
    "        if use_cuda:\n",
    "            imgs = imgs.cuda()\n",
    "            embeddings = embeddings.cuda()\n",
    "        \n",
    "        code = model.encoder(imgs)\n",
    "        recon1 = model.decoder(code) #autoencoder\n",
    "        recon2 = model.decoder(embeddings) #decoder from ground truth\n",
    "        \n",
    "\n",
    "        loss = 1*mse(code, embeddings) + 1*l1(code, embeddings)\n",
    "        loss += 100*mse(recon1, imgs) + 50*l1(recon1, imgs)\n",
    "#         loss += 4*mse(recon2, imgs) + 2*l1(recon2, imgs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epochBar.set_postfix({'loss': loss.item()})\n",
    "        lossHistory.append(np.log(loss.item()))\n",
    "        \n",
    "    if epoch==0 or epoch % 10 == 9:\n",
    "        with torch.no_grad():\n",
    "            display.clear_output(wait=True)\n",
    "            plt.figure(figsize=[20,6])\n",
    "            \n",
    "            plt.subplot(gs[:,0])\n",
    "            plt.plot(lossHistory)\n",
    "            plt.title('loss')\n",
    "            \n",
    "            plt.subplot(gs[:,1])\n",
    "            for imgs, embeddings, labels in test_loader:\n",
    "                if use_cuda:\n",
    "                    code = model.encoder(imgs.cuda()) \n",
    "                else:\n",
    "                    code = model.encoder(imgs)\n",
    "                    \n",
    "                recon = model.decoder(code)\n",
    "                code = code.cpu().numpy()\n",
    "                plt.scatter(embeddings[:,0], embeddings[:,1], s=1, c='#333333', zorder=1)\n",
    "                plt.scatter(code[:,0], code[:,1], s=1, c=labels, cmap='tab10', zorder=2)\n",
    "                \n",
    "            plt.colorbar()\n",
    "            plt.axis('equal')\n",
    "            plt.title('learned embedding (epoch {}/{})'.format(epoch, nepoch))\n",
    "#             plt.xlim([-8,6])\n",
    "#             plt.ylim([-4,6])\n",
    "            \n",
    "            plt.subplot(gs[0,2])\n",
    "            imshowGrid(imgs)\n",
    "            plt.title('original')\n",
    "            plt.subplot(gs[1,2])\n",
    "            imshowGrid(recon)\n",
    "            plt.title('learned')\n",
    "            plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload embeddings, get parameters for frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.fromfile('tmp/embeddings.bin', dtype=np.float32).reshape([-1,2])\n",
    "imgs = np.fromfile('tmp/imgs.bin', dtype=np.float32).reshape([-1,28,28])\n",
    "labels = np.fromfile('tmp/labels.bin', dtype=np.uint8)\n",
    "\n",
    "[xmin, ymin] = embeddings.min(axis=0)\n",
    "[xmax, ymax] = embeddings.max(axis=0)\n",
    "vmin = torch.tensor([xmin, ymin])\n",
    "vmax = torch.tensor([xmax, ymax])\n",
    "\n",
    "print(xmin, xmax, ymin, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump data to js files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './reconstruct-fashion/'\n",
    "!rm -r $data_dir\n",
    "!mkdir $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'constants.js', 'w') as f:\n",
    "    f.write('let constants = {};\\n')\n",
    "    \n",
    "    f.write('constants.xrange = \\n')\n",
    "    json.dump([float(xmin), float(xmax)], f)\n",
    "    f.write(';\\n')\n",
    "    \n",
    "    f.write('constants.yrange = \\n')\n",
    "    json.dump([float(ymin), float(ymax)], f)\n",
    "    f.write(';\\n')\n",
    "\n",
    "with open(data_dir+'data.js', 'w') as f:\n",
    "    f.write('let data = {}; \\ndata.embeddings = \\n')\n",
    "    json.dump(embeddings.tolist(), f)\n",
    "    f.write('\\n\\n')\n",
    "    f.write('data.labels = \\n')\n",
    "    json.dump(labels.tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to a tfjs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "## define a kera model that is equicalent to the pytorch decoder\n",
    "model_keras = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(784, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "## copy the weights\n",
    "print([w.shape for w in model_keras.get_weights()])\n",
    "weights = [[model.decoder[i].weight, model.decoder[i].bias]  for i in [0,2,4]]\n",
    "weights = sum(weights, [])\n",
    "weights = [w.data.cpu().numpy().T for w in weights]\n",
    "model_keras.set_weights(weights)\n",
    "\n",
    "# model_keras.save('model_keras.h5')\n",
    "tfjs.converters.save_keras_model(model_keras, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress into a zip (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = data_dir[:-1] + '.zip'\n",
    "!rm $zip_filename\n",
    "!zip -r $zip_filename $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
